System Prompts and Types of Prompting

Hey there everyone, this is Tushar, and in today's blog, we are going to take a deep dive into Prompting in AI.

Prompting is undoubtedly one of the most important parts of LLMs.

In tech, there is a rule called GIGO, which means "Garbage In, Garbage Out."

This means the quality of the output you receive is directly tied to the quality of the input you provide. This works exactly the same way with LLMs.

The better quality prompt you give, the better quality output you receive. And guess what? That‚Äôs exactly what Prompt Engineering is all about.

But before we get into that, we should first understand prompts, the types of prompts, and what makes a good prompt.

So without any further delay, let‚Äôs get started.

Have you ever gone to GPT and asked it something like, "Explain this complex code step-by-step"?

Yeah, that‚Äôs exactly a prompt. It means whatever you give the LLM as your input text is considered a prompt.
In general use cases, we may not think much about the importance of prompts because LLMs are powerful enough to give quality answers to simple questions, even with low-quality prompts.

But in the professional world, especially in GenAI, prompts‚Äîand the quality of those prompts‚Äîmake a huge difference.

Let‚Äôs take an example.

Suppose I go to GPT and ask:
"Hey GPT, give me the program for finding a prime number in JavaScript."

Now, imagine I open a new conversation and say:
"Hey, for this chat, you are my personal JS Trainer. You know nothing beyond JavaScript, and you are a JavaScript expert. If I ask anything unrelated to JavaScript, please don‚Äôt answer me."

Then I paste the same prime number prompt into this conversation.

Do you think there‚Äôs a difference?

Yes, absolutely! Now GPT is acting like a JS expert. This is just one example.

You can go further, providing more context to the model to get even more fine-tuned responses.

This is known as a System Prompt‚Äîproviding context to the LLM so that it can respond exactly the way you want. This is one of the best ways to improve output quality.

Types of System Prompting
1. Zero-Shot Prompting

The model is directly given a task or query without any prior examples.

2. Few-Shot Prompting

You give the LLM examples showing how it should respond to a particular kind of query.

For example, in the system prompt, you could write:

*"Hey, for this chat, you are my personal JS Trainer. You know nothing beyond JavaScript, and you are a JavaScript expert. If I ask anything unrelated to JavaScript, please don‚Äôt answer me.

Any reply you give must start with this emoji: üëç

Example:
User: 'Hey GPT, give me the program for finding a prime number in JavaScript.'
System: 'üëç Sure, here‚Äôs the code for finding a prime number in JavaScript.'"*

This is how you can train an LLM for customized outputs.

3. Chain of Thought (CoT) Prompting

This is one of the most widely used prompting techniques for LLMs.
Think of it like an LLM thinking to itself‚Äîanalyzing and reasoning before giving the output. This can significantly improve accuracy.

Humans do this too: when we want to be sure we don‚Äôt make mistakes, we think carefully before speaking.

The CoT algorithm makes the LLM follow a similar step-by-step thought process to enhance output accuracy.

If you‚Äôve used features like "DeepThink", "DeepSearch", or "Think Longer" in LLM tools, that‚Äôs exactly the idea behind them.

To implement this, you just modify the system prompt.
When you click on the DeepSearch button, you‚Äôre essentially adding this instruction behind the scenes.

Here‚Äôs an example CoT prompt:

"You are an AI assistant that works in a step-by-step process: start, think, and output.
Before answering any user query, think through the topic thoroughly.
Always proceed step-by-step, moving to the next step only after completing the current one.
You may think multiple times between the start and output in detail.
You can only provide one step of the answer at a time.
Always present your output in a way that shows detailed thinking."

This is just a small example. Professionals often use much larger system prompts with multiple examples.

Persona-Based Prompting

Persona-based prompting is when you design the chatbot to behave like a specific identity.

Have you ever seen an AI bot of a particular person on their website?
That‚Äôs one of the best implementations of persona-based prompting.

It‚Äôs widely used in EdTech, where users are made to feel as if a person is available 24/7 to answer their questions.

Again, to implement this, you modify the system prompt.

Here‚Äôs an example persona prompt for Hitesh Choudhary from Chai aur Code:

*"You are an AI persona of a man named Hitesh Choudhary. Hitesh is a nice guy, an excellent coder, a brilliant teacher, and an entrepreneur with years of experience in the EdTech industry.

He is generally chill, a great YouTuber, and very passionate about JavaScript and Python. His Hindi channel, 'Chai aur Code,' has over 700k subscribers. He also runs an English channel called 'Hitesh Choudhary.'

He is retired from corporate roles but is still highly respected in the community.

Here‚Äôs more about him: 'Retired from corporate, full-time YouTuber, ex-founder of LCO (acquired), ex-CTO, Sr. Director at PW. Two YT channels (950k & 470k), visited 43 countries.'

LinkedIn: www.linkedin.com/in/hiteshchoudhary

Hitesh‚Äôs speaking style in 'Chai aur Code' is friendly, engaging, and carries a bit of desi swag. He maintains a casual vibe, often mentioning chai to make concepts relatable. He motivates beginners and explains technical topics in simple language.

He generally speaks in Hindi and starts videos with: 'Haanji, to kaise hai aap sabhi?'

Here are some example statements from him to understand his vocabulary better:"*

Intro Style:
"Hanji, namaste doston! Swagat hai aapka ek aur naye video mein, Chai aur Code ke saath. Aaj hum banayenge ek mast project, toh apni chai ka cup uthao aur chalo shuru karte hain!"

Motivating Students:
"Doston, coding mein galtiyan toh banta hai boss! Tum galti karoge, usse seekhoge, aur ek din pro ban jaoge. Bas apni chai thandi nahi hone dena, aur code chalu rakho!"

Engaging with Audience:
"Ab yeh wala code dekho, kya lagta hai, ye kaam karega ya nahi? Comment mein batao, aur agar stuck ho toh mujhe bolna, hum saath mein debug karenge, chai ke saath!"

Ending a Session:
"Bas doston, aaj ke liye itna hi. Agar video pasand aaya toh like karo, channel subscribe karo, aur apni chai ke saath agla video wait karo. Chaliye milte hai fir next time!"

That‚Äôs how you can train a bot to respond like a specific person.

The more text, examples, and context you provide, the better the output.

Pro Tip: While designing persona bots, include tweets, video transcripts, or social media posts from the personality. This helps the LLM better capture their tone and style.

So that‚Äôs it from my side today.
Hope you enjoyed the blog.
Thank you, and goodbye!